%! Author = Len Washington III
%! Date = 8/28/24

% Preamble
\documentclass[
	number={2},
	title={Learning Linear Separators{,} SVMs and Kernels}
]{cs584notes}

% Document
\begin{document}

%\maketitle

\tableofcontents

\section{Linear Separator}\label{sec:linear-separator}
Assuming that \textcolor{red}{red} and \textcolor{blue}{blue} datasets represents points \data{$X_{1}$} and \data{$X_{2}$}, then the two sets \data{$X_{1}$} and \data{$X_{2}$} are \emph{linearly separable} if there exists $(n+1)$ real numbers \data{$w_{1}, w_{2}, \dots, w_{n}, k$}
\begin{itemize}
	\item such that every point in \data{$X_{1}$} \emph{satisfies} $ \sum_{i=1}^{n} w_{i}x_{i} < k $
	\item such that every point in \data{$X_{2}$} \emph{satisfies} $ \sum_{i=1}^{n} w_{i}x_{i} > k $
\end{itemize}

Binary \emph{classification} \data{$y_{i} \in \{-1, 1\}$} can be viewed as the task of \emph{separating classes} in feature space.
\begin{itemize}
	\item Hypothesis class of linear \emph{decision surfaces} is \data{$f(x_{i}) = \sign(\vec{w}^{T}\vec{x_{i}} + b)$.}
	\item Without loss of generality, we assume that $b=0$. Thus, we get the simplified \data{$f(x_{i}) = \sign(\vec{w}^{T}\vec{x_{i}})$.}
	\item \data{$(y_{i})(\vec{w}^{T}\vec{x}_{i}) > 0$} $\Leftrightarrow$ data point \data{$x_{i}$} is correctly \emph{classified.}
	\begin{itemize}
		\item Remember, \data{$y_{i}$} is counting as \data{1} or \data{-1}.
	\end{itemize}
\end{itemize}

\section{Perceptron Algorithm}\label{sec:perceptron-algorithm}
\begin{itemize}
	\item Set time \data{$t=1$}, start with vector \data{$\vec{w}_{1}=\oldvec{0}$}.
	\item Given example $\vec{x}$, \emph{predict positive} iff (if and only if) \data{$\vec{w_{1}\cdot\vec{x} \geq 0}$}.
	\item On a mistake, update as follows:
	\begin{itemize}
		\item Mistake on \emph{positive}, then update \data{$\vec{w}_{t+1} \gets \vec{w}_{t} + \vec{x}$}.
		\item Mistake on \emph{negative}, then update \data{$\vec{w}_{t+1} \gets \vec{w}_{t} - \vec{x}$}.
	\end{itemize}
\end{itemize}

\section{Geometric Margin}\label{sec:geometric-margin}
The \emph{margin} of example $\data{}$ $\dots$

The \emph{margin} \data{$\gamma$} of a set of examples $\data{S}$ w.r.t (with respect to)  a \emph{linear separator} \data{$\vec{w}$} is the largest margin over points \data{$\vec{x}\in S$}.
Theorem: If the data has a \emph{margin} \data{$\gamma$} and all points lie inside a ball of \emph{radius} \data{$R$}, then the Perceptron algorithm makes \data{$\leq \frac{R}{\gamma^{2}}$} \emph{mistakes.}

\section{Support Vector Machine}\label{sec:svm}
\emph{Support vector machines} (SVMs) are \emph{supervised max-margin} models with associated learning algorithms.
\begin{itemize}
	\item Good \emph{generalization} in theory.
	\item Good \emph{generalization} in practice.
	\item Work well with \emph{few training instances}.
	\item Find \emph{globally best} model.
	\item \emph{Efficient algorithms}.
	\item Amenable to the \emph{kernel trick}.
\end{itemize}

\section{Optimal Linear Separator}\label{sec:optimal-linear-separator}
Which of the \emph{linear separators} is optimal?
% TODO: Insert figure of linear separators from notes

\section{Classification Margin}\label{sec:classification-margin}
Examples closest to the hyperplane are \emph{support vectors}.
Margin \data{$\rho$} of the separator is the \emph{distance between support vectors}.

\section{Maximizing the Margin}\label{sec:maximizing-the-margin}
\begin{itemize}
	\item Better \emph{Generalization} -- A larger margin allows the SVM to \emph{better generalize} to new, unseen data, leading to \emph{higher predictive accuracy}.
	\item Improved \emph{Robustness} -- A larger margin can lead to improved robustness \emph{against noise and outliers} in the training data, as it allows for \emph{greater tolerance} of misclassified examples.
	\item Reducing \emph{Overfitting} -- A larger$\dots$
\end{itemize}

\section{Linear SVM}\label{sec:linear-svm}
Let training set \data{$\left\{ (\vec{x}_{i}, y_{i})_{i=1\dots{}n}\ , \vec{x}_{i}\in\mathbb{R}^{d}, y_{i} \in \{ -1, 1\} \right\}$} be separated by a hyperplane with margin \data{$\rho$}.
Then for each training example \data{$(\vec{x}_{i}, y_{i})$}

\begin{equation*}
\begin{aligned}
	\vec{w}^{T}\vec{x}_{i} + b \geq 1 && \mbox{ if } y_{i} = 1 &&\\
	&&&& \Leftrightarrow y_{i}\left( \vec{w}^{T}\vec{x}_{i} + b \right) \geq 1\\
	\vec{w}^{T}\vec{x}_{i} + b \leq -1 && \mbox{ if } y_{i} = -1 &&\\
\end{aligned}
\end{equation*}

Geometrically, the \emph{distance} between the \emph{2 hyperplanes} can be expressed as:
\begin{equation}
	\rho = \frac{2}{||w||}
	\label{eq:hyperplane-distance}
\end{equation}

Then we can formulate the \emph{quadratic optimization problem}:

\begin{svmbox}
	Find \data{$\vec{w}$} and \data{$b$} such that \data{\[ \rho = \frac{2}{||\vec{w}||} \]} is \emph{maximized} \emph{and} for all \data{$\left( \vec{x}_{i}, y_{i} \right), i=1\dots{}n : y_{i}\left( \vec{w}^{T}\vec{x}_{i} + b \right) \geq 1$}
\end{svmbox}

Which can be reformulated as:

\begin{svmbox}
	\data{$\vec{x}_{i}, y_{i}$}, find \data{$\vec{w}$} and \data{$b$} such that
	\data{\[ \mbox{\emph{Minimize }} Q(w) = \frac{1}{2}||\vec{w}||^{2} = \frac{1}{2}\vec{w}^{T}\vec{w} \]}
	subject to \data{$~$} $\dots$ % TODO: Add green box around the content
\end{svmbox}

\section{Lagrangian Duality}\label{sec:lagrangian-duality}
\begin{itemize}
	\item Need to \emph{optimize} a \emph{quadratic function} subject to \emph{linear constraints}.
	\item Quadratic optimization problems are a well-known class of mathematical programming problems for which several (non-trivial) algorithms exist.
	\item Solution involves \emph{constructing dual problem} where \emph{Lagrange multipliers} \data{${a_{i}}$} is associated with all inequality constraint in primal (original) problem:
\end{itemize}

\begin{svmbox}
	$\forall i$, find \data{$a_{1},\dots,a_{n}$} such that ... subject to \data{$a_{i} \geq 0$}
\end{svmbox}

\section{SVM Solution}\label{sec:svm-solution}
\begin{itemize}
	\item Given a \emph{solution} \data{$a_{1}\dots a_{n}$} to the dual problem, solution to the primal is:
	\begin{svmbox}
		\data{\[ \vec{w} = \sum a_{i}y_{i}\vec{x}\ \ \ b = y_{k} - \sum a_{i}y_{i}\vec{x}_{i}^{T}\vec{x}_{k} \mbox{ for any } a_{k} > 0  \]}
	\end{svmbox}
	\item Each non-zero $a_{i}$ indicates that corresponding \data{$\vec{x}_{i}$} is a \emph{support vector}.
	\item Then the \emph{classifying function is:}
	\begin{svmbox}
		\[ \data{ f(x) = \sum a_{i}y_{i}\vec{x}_{i}^{T}\vec{x} + b } \]
	\end{svmbox}
	\item Notice that it relies on the \emph{inner product} between the test point \data{$\vec{x}$} and the support vectors \data{$\vec{x}_{i}$}.
	\item Solving the optimization problem involves \emph{computing the inner products} \data{$\vec{x}_{i}^{T}\vec{x}_{j}$} between all training points.
\end{itemize}

\section{Soft Margin Classification}\label{sec:soft-margin-classification}
\begin{itemize}
	\item What if the training set is \emph{not linearly separable?}
	\item \emph{Slack variables} can be added to \emph{allow misclassification} of difficult or noisy examples; thus, the result margin is called \emph{soft}.
\end{itemize}

\section{Kernel Method}
\begin{itemize}
	\item If we \emph{map} the \emph{input vectors} into a very \emph{high-dimensional feature space}, the task of \emph{finding} the \emph{maximum-margin separator} can become \emph{computationally intractable}.
	\item All of the \emph{computations} that we need to do to find the \emph{maximum-margin separator} (SVM optimization problem) can be expressed in terms of \emph{inner products} between \emph{pairs of data points} (in the high-dimensional feature space).
	\item These \emph{inner products} are the only part of the computation that depends on the dimensionality of the high-dimensional space.
	So, if we had a \emph{fast way} to do the dot products, we would \emph{not have ot pay a price}
	\item The \emph{kernel trick} is just a way of \emph{doing inner products} a whole lot faster than is usually possible.
	It relies on \emph{choosing a way of mapping} to the \emph{high-dimensional feature space} that allows fast scalar products.
	\item By using a \emph{nonlinear vector function} \data{$\phi(x) = \left< \phi(x_{1}), \dots, \phi(x_{n}) \right>$}, the \data{$n$}-dimensional input vector \data{$\vec{x}$} can be mapped into high-dimensional feature space.
	The \emph{decision function} in the feature space is expressed as:
	\[ \data{ f(x) = \vec{w}^{T}\phi(\vec{x}) + b } \]
	\item In terms of solving the \emph{quadratic optimization problem of SVM}, each training data point is in the form of dot products.
	A \emph{kernel function} \data{$K$} simplifies the calculation of dot product terms
	\[ \data{K(\vec{x_{1},)}} \] % TODO: Finish me

\end{itemize}

\section{Kernel Trick}
Example: Take this 2-dimensional vector: $\vec{x} = [x_{1}, x_{2}]$
\begin{equation*}
	\begin{aligned}
		K(\vec{x}_{i}, \vec{x}_{j}) &= (1 + \vec{x}_{i}^{T}\vec{x}_{j})^{2}\\
		&= 1 + x_{i1}^{2}x_{j1}^{2} + 2x_{i1}x_{j1}x_{i2}x_{j2} + x_{i2}^{2}x_{j2}^{2} + 2x_{i1}x_{j1} + 2x_{i2}x_{j2}\\
		&= \left[ 1\ x_{i1}^{2} \sqrt{2}x_{i1}  \right]
	\end{aligned}
\end{equation*}

\end{document}
