%! Author = Len Washington III
%! Date = 9/18/24

% Preamble
\documentclass[
	title={Linear Regression}
]{cs584notes}
\usepackage{pas-tableur}

% Document
\begin{document}

%\maketitle

\tableofcontents

\section{Definition}\label{sec:definition}
\begin{itemize}
	\item In a \emph{supervised learning} problem, given the \emph{input variables} \data{$X$} and \emph{outputs} \data{$Y$}, the goal of \emph{linear regression} is to \emph{learn} a \emph{function} that can \emph{predict} an output given an input.
	\item We find the best line (linear function \data{$y=f(X)$}) to explain the data.
\end{itemize}

\section{Examples}\label{sec:examples}
Predicting a continuous outcome variable
\begin{itemize}
	\item Predicting a company's future stock price using its profit and other financial information.
	\item Predicting \emph{annual rainfall} based on local flora and fauna.
	\item Predicting \emph{distance} from a traffic light using LIDAR measurements.
\end{itemize}

\section{Simplest Linear Regression}\label{sec:simplest-linear-regression}
\begin{itemize}
	\item \data{$x$} is an input feature.
	\item \data{$y$} is the value we're trying to predict.
	\item The \emph{regression model} is:
	\[ \data{ y = w_{1}x + w_{0} } \]
	\item Two parameters to estimate --
	\begin{itemize}
		\item the slope of the line \data{$w_{1}$},
		\item the \data{$y$}-intercept \data{$w_{0}$}.
	\end{itemize}
	\item We basically want to find \data{$\{w_{0}, w_{1}\}$} that \emph{minimize deviations} from the predictor line.
	\data{\begin{equation*}
	\begin{aligned}
		\min&\sum_{i=1,2,\dots,n} (y_{i} - \hat{y}_{i})^{2}\\
		=\min_{w_{0},w_{1}}&\sum_{i=1,2,\dots,n} (y_{i} - w_{1}x_{i} - w_{0})^{2}\\
	\end{aligned}
	\end{equation*}}
\end{itemize}

\section{Linear Regression Function Model}\label{sec:linear-regression-function-model}
Function \data{$f: X\rightarrow Y$} is a linear combination of input components
\begin{equation*}
\begin{aligned}
	f(x) &= w_{0} + w_{1}x_{1} + w_{2}x_{2} + \dots + w_{d}x_{d} = w_{0} + \sum_{j=1}^{d} w_{j}x_{j}
\end{aligned}
\end{equation*}
\data{$w_{0}, w_{1}, \dots, w_{d}$} are the \emph{parameters} (weights)

\section{Error}\label{sec:error}
\begin{itemize}
	\item \emph{Error} function measures how much our predictions deviate from the \emph{desired answers}.
	\item Mean-squared error (MSE):
	\data{\begin{equation}
		\begin{aligned}
			J_{n} &= \frac{1}{2n}\sum_{i=1}^{n} (y_{i} - f(\vec{x}_{i}))^{2}\\
				  &= \frac{1}{2n}\sum_{i=1}^{n} (y_{i} - \vec{w}^{T}\vec{x}_{i})^{2}\\
		\end{aligned}
		\label{eq:mse}
	\end{equation}}
	\item Learning: We want to find the \emph{weights minimizing the error}.
	\item In~\eqref{eq:mse}, \data{$y_{i} - \vec{w}^{T}\vec{x}_{i}$} is the \emph{residual\label{dfn:residual}} and $\dots$.
\end{itemize}

\section{Optimization}\label{sec:optimization}
\begin{itemize}
	\item For the optimal set of parameters, \emph{derivatives} of the \emph{error} with respect to each \emph{parameter} must be \data{$0$}.
	\data{\begin{equation*}
	\begin{aligned}
		\frac{\partial}{\partial w_{j}} J_{n}(\vec{w}) &= -\frac{1}{n} \sum_{i=1}^{n} (y_{i} - w_{0}x_{i0} - w_{1}x_{i1} - \dots - w_{d}x_{id})x_{ij}\\
		&= 0
	\end{aligned}
	\end{equation*}}
	\item Vector of derivatives:
	\data{\begin{equation*}
	\begin{aligned}
		\nabla_{w} &= -\frac{1}{n} \sum_{i=1}^{n} \left( y_{i} - \vec{w}^{T}\vec{x}_{i} \right)\vec{x}_{i}\\
				   &= \vec{0}
	\end{aligned}
	\end{equation*}}
	\item By rearranging the terms, we get a system of linear equations with \data{$d+1$} unknowns.
	\[ \data{ w_{0}\sum_{i=1}^{n} x_{i0}\cdot x_{ij} + w_{1}\sum_{i=1}^{n} x_{i1}\cdot x_{ij} + \dots + w_{d}\sum_{i=1}^{n} x_{id}\cdot x_{ij} + \sum_{i=1}^{n} y_{i}\cdot x_{ij} } \]
	\item Can also be solved through matrix inversion if the matrix is not singular.
	\[ \data{ \vec{A}\vec{w} = \vec{b} \Rightarrow \vec{w} = \vec{A}^{-1}\vec{b} } \]
\end{itemize}

\section{Linear Regression as a System of Linear Equations}\label{sec:linear-regression-as-a-system-of-linear-equations}
The \emph{linear regression model} is akin to a \emph{system of linear equations}.
Assuming \data{$n$} training examples with \data{$d+1$} \emph{features} each --
\begin{equation*}
\begin{aligned}
	\data{1^{\mbox{st}}} & \mbox{ training example:} & y_{1} = w_{0} + x_{11}w_{1} + x_{12}w_{2} + \dots + x_{1d}2_{d}\\
	\data{2^{\mbox{nd}}} & \mbox{ training example:} & y_{2} = w_{0} + x_{21}w_{1} + x_{22}w_{2} + \dots + x_{2d}2_{d}\\
	 & \vdots & \\
	\data{n^{\mbox{th}}} & \mbox{ training example:} & y_{n} = w_{0} + x_{n1}w_{1} + x_{n2}w_{2} + \dots + x_{nd}2_{n}\\
\end{aligned}
\end{equation*}

\section{Solving Linear Regression}\label{sec:solving-linear-regression}
\subsection{Using Matrices}\label{subsec:using-matrices}
\begin{itemize}
	\item \data{$J_{n}(\vec{w})$} can be rewritten in terms of data \emph{matrices} \data{$X$} and \emph{vectors}:
	\begin{equation*}
	\begin{aligned}
		J_{n}(\vec{w}) &= \frac{1}{2}(\dots)
	\end{aligned}
	\end{equation*}
\end{itemize}

\subsection{Using Gradient Descent}\label{subsec:using-gradient-descent}
\begin{itemize}
	\item Linear regression problem comes down to the problem of solving a set of linear equations:
	\data{\begin{equation*}
	\begin{aligned}
		\vec{w} &\gets \vec{w} - \eta\cdot\nabla_{\vec{w}} J_{n}(\vec{w})\\
		\nabla J_{n}(\vec{w}) &= -\vec{X}^{T}(\vec{y} - \vec{X}\vec{w})\\
		\vec{w} &\gets \vec{w} - \eta\cdot\vec{X}^{T}(\vec{X}\vec{w} - \vec{y})\\
	\end{aligned}
	\end{equation*}}
\end{itemize}

\section{Linear Regression}\label{sec:linear-regression}
\begin{itemize}
	\item The \emph{error function} defined for the whole dataset for the linear regression is:
	\[ \data{ J_{n} = \frac{1}{2n}\sum_{i=1}^{n} (y_{i} - f(\vec{x}_{i}))^{2} } \]
	\item \emph{Online Gradient Descent}: use the \emph{most recent sample} at each iteration.
	Instead of MSE for all data points, it uses \emph{MSE} for an \emph{individual sample}.
	\data{\begin{equation*}
	\begin{aligned}
		J_{online} &= Error_{i}(\vec{w})\\
				   &= \frac{1}{2} \left( y_{i} - f(\vec{x}_{i}) \right)^{2}\\
		\vec{w} \gets \vec{w} - \eta\dots
	\end{aligned}
	\end{equation*}}
\end{itemize}

\section{Input Normalization}\label{sec:input-normalization}
\begin{itemize}
	\item Makes the \emph{data} very roughly on the \emph{same scale}.
	\item Can make a huge difference in \emph{online learning}.
	\item For \emph{inputs} with a \emph{large magnitude}, the \emph{change} in the \emph{weight} is \emph{huge}.
	\item \emph{Solution}: Make all inputs vary in the same range.
	\item New output:
	\[ \data{ \hat{x_{ij}} = \frac{x_{ij} - \bar{x}_{j}}{\sigma_{j}} } \] % TODO: Make the hat on the x a ~
\end{itemize}

\section{L1/L2 Regularization}\label{sec:l1/l2-regularization}
Using \emph{L1/L2 Regularization}, we can rewrite our loss function as:
\data{\begin{equation*}
\begin{aligned}
	L_{lasso} &= \frac{1}{2n}\sum_{i=1}^{n} \left( y_{i} - f(\vec{w}^{T}\vec{x}_{i}) \right)^{2} + \lambda ||\vec{w}||_{1}\\
	L_{ridge} &= \frac{1}{2n}\sum_{i=1}^{n} \left( y_{i} - f(\vec{w}^{T}\vec{x}_{i}) \right)^{2} + \lambda ||\vec{w}||_{2}^{2}\\
\end{aligned}
\end{equation*}}

\section{Other Ways to Control Overfitting}\label{sec:other-ways-to-control-overfitting}
\begin{description}[font=\emph]
	\item[Early-stopping]: stopping training when a monitored matrix has stopped improving.
	\item[Bagging]: learning multiple models in parallel and applying majority voting to choose final predictor.
	\item[Dropout]: in each iteration, don't update some of the weights.
	\item[Injecting noise] in the inputs.
\end{description}

\section{Bias-Variance Tradeoff}\label{sec:bias-variance-tradeoff}
\begin{itemize}
	\item \emph{Bias} captures the \emph{inherent error} present in the model.
	The \emph{bias error} originates from \emph{erroneous assumption(s)} in the \emph{learning algorithms}.
	\item \emph{Bias} is the \emph{contrast} between the \emph{mean prediction} of our model and the \emph{correct prediction}.
	\item \emph{Variance} captures how much the \emph{model changes} if it is \emph{trained} on a \emph{different training set}.
	\item \emph{Variance} is the variation or spread of \emph{model prediction} values across \emph{different data samples}.
	\item \emph{Underfitting} happens when a \emph{model unable to capture} the underlying \emph{pattern} of the data.
	Such models usually have \emph{high bias} and \emph{low variance.}
	\item It usually happens when there is much fewer amount of \emph{data} to build an accurate model or when a \emph{linear model} is used to \emph{learn non-linear data}.
	\item \emph{Overfitting} happens when our \emph{model captures the noise} along with the underlying pattern in \emph{data}.
	\item $\dots$
\end{itemize}

Bias:
\begin{equation}
	(y - \hat{y})
	\label{eq:bias}
\end{equation}

Variance:
\begin{equation}
	\frac{1}{k-1} \sum_{j=1}^{k-1} \left( \hat{y}_{j} - \hat{y} \right)^{2}
	\label{eq:variance}
\end{equation}

Total Error:
\begin{equation}
	TE = Bias^{2} + Variance = (y - \hat{y})^{2} + \frac{1}{k-1} \sum_{j=1}^{k-1} \left( \hat{y}_{j} - \hat{y} \right)^{2}
	\label{eq:total-error}
\end{equation}

\[ \data{ \mbox{ Expected Loss = Total Error = Bias}^{2} + \mbox{Variance } } \]

\end{document}