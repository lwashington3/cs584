%! Author = Len Washington III
%! Date = 10/16/24

% Preamble
\documentclass[
	number={8},
	title={Artificial Neural Network}
]{cs584notes}

% Document
\begin{document}

\section{ANN Properties}\label{sec:ann-properties}
\begin{itemize}
	\item ANN is much like a \emph{black-box}.
	\item Many \emph{neuron-like} threshold \emph{switching} units.
	\item Many \emph{weighted interconnections} amount units.
	\item Highly \emph{parallel} and \emph{distributed} process.
\end{itemize}

\section{Perceptron}\label{sec:perceptron}
Linear function: \data{$f: X \rightarrow Y$}
\begin{equation*}
\begin{aligned}
	f(\vec{x}) &= w_{0} + w_{1}x_{1} + w_{2}x_{2} + \dots + w_{d}x_{d}\\
	&= w_{0} + \sum_{j=1}^{d} w_{j}x_{j}
\end{aligned}
\end{equation*}
\data{$w_{0}, w_{1}, \dots, w_{d}$} are weights

\section{Multi-layer Perceptron}\label{sec:multi-layer-perceptron}
\begin{itemize}
	\item Multi-layer perceptrons (MLPs) were designed to \emph{overcome} the \emph{computational limitation} of a single threshold element (\emph{perceptron}).
	\item The idea is to \emph{stack several layers} of threshold elements, each layer using the output of the previous layer as input.
	\item A \emph{feed-forward MLP} network defines a mapping:
	\[\data{ y = f(x; \theta) }\]
	\item \emph{Functions} are composed in a \emph{chain}:
	\[\data{ f(x) = f_{3}(f_{2}(f_{1}(x))) }\]
	\item \emph{Input layer}: The process starts with the input layer, which receives the input data.
	Each \emph{neuron} in this \emph{layer} represents a \emph{feature} of \emph{input data}.
	\item \emph{Weights and Biases}: \emph{Connections} between \emph{neurons} have associated \emph{weights}, which are \emph{learned} during the training process and is crucial to capture patterns in the data.
	\item \emph{Hidden Layers}: The neurons in these layers perform computations on the inputs.
	The \emph{output} of each \emph{neuron} is calculated by applying a \emph{weighted sum of its inputs} (from the previous layer).
	\item \emph{Activation Functions}: The activation function is crucial as it introduces \emph{non-linearity} into the model, allowing it to \emph{learn} more \emph{complex} functions. $\dots$
\end{itemize}

\section{Activation Functions}\label{sec:activation-functions}
\begin{itemize}
	\item \emph{Non-linearity}: This is the most fundamental property; activation functions introduce nonlinearity into the network.
	This is important because \emph{real-world relationships and patterns} are rarely linear.
	\item $\dots$
	\item \emph{Monotonicity}: A monotonic activation function either \emph{strictly increases} or \emph{strictly decreases} as \emph{input values change}.
	This property ensures that as inputs change, the \emph{neuron's output} moves in a \emph{consistent direction}.
	\item \emph{Continuity}: A continuous activation function produces \emph{smooth and continuous changes} in output as inputs change slightly.
	This property helps in \emph{smooth gradient computations} for \emph{updating weights} during the \emph{learning process}.
	\item \emph{Differentiability}: Differentiability is essential for \emph{gradient-based optimization} algorithms like backpropagation.
	\emph{Activation functions} that are \emph{differentiable} across their domain allow \emph{gradients} to be computed for \emph{weight updates} during training.
	\item \emph{Sparsity}: Some activation functions promote sparsity by having their \emph{outputs} be \emph{zero} for a large portion of input space.
	This can be beneficial in \emph{reducing} the \emph{complexity} of neural networks.
\end{itemize}
Rectified Linear Units (ReLU):
\begin{equation}
	\begin{aligned}
		g(x) &= \max\{0, x\}\\
		g'(x) &= \left\{ \begin{array}{cl}
			1, & x \geq 0\\
			0, & x < 0
		\end{array} \right.
	\end{aligned}
	\label{eq:relu}
\end{equation}

Sigmoid:
\begin{equation}
	\begin{aligned}
		g(x) &= \frac{1}{1 + e^{-x}}\\
		g'(x) &= g(x)\left( 1 - g(x) \right)
	\end{aligned}
	\label{eq:sigmoid}
\end{equation}

Hyperbolic tangent $\tanh$
\begin{equation}
	\begin{aligned}
		g(x) &= \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\\
		g'(x) &= 1 - g(x)^{2}
	\end{aligned}
	\label{eq:tanh}
\end{equation}

\begin{table}[H]
	\centering
	\caption{Activation Functions}
	\label{tab:activation-functions}
	\begin{tabular}{l l l l}
		\textbf{Activation Functions} & \textbf{Sigmoid} & \textbf{tanh} & \textbf{ReLU}\\
		Range & $(0, 1)$ & $(-1, 1)$ & $[0, \infty)$ \\
		Vanishing Gradient Problem & Yes & Yes & No\\
		Nature & Non-Linear & Non-Linear & Linear\\
		Zero Centered Activation Function & No & Yes & No\\
		Symmetric Function & No & Yes & No
	\end{tabular}
\end{table}

\section{Feed-forward MLP}\label{sec:feed-forward-mlp}
\begin{itemize}
	\item The \emph{weights} of the \emph{neural network connections} are \emph{repeatedly adjusted} to \emph{minimize} the \emph{difference} between the \emph{actual output} and \emph{desired output}.
	\item Aims to \emph{minimize} the \emph{loss function} by \emph{adjusting} the \emph{network's weights and biases}.
	The \emph{loss function gradients} determine the level of adjustment with respect to parameters like activation function, weights, bias, etc.
	\item The \emph{forward step}, given the input, \emph{computers} the \emph{output layer-by-layer}, starting with the \emph{input layer}.
	\item The \emph{backward} step \emph{calculates} the \emph{error} in the output and \emph{propagates} it \emph{backwards}; then \emph{update} the \emph{weights layer-by-layer}, starting from the output layer.
\end{itemize}

\section{Forward Pass}\label{sec:forward-pass}
\begin{itemize}
	\item In this example, we will be using a \emph{three-layer neural network} with the layers being the input, hidden, and output layers.
	\item The \emph{activation} $\dots$
\end{itemize}

\section{Gradient Descent}\label{sec:gradient-descent-review}
\begin{itemize}
	\item Goal of NN: Given \data{$n$} training samples \data{$(x_{i}, y_{i})$}, find \data{$w$} to \emph{minimize}:
	\begin{equation}
		E[w] = \frac{1}{2n} \sum_{i=1}^{n} (y_{i} - o_{i})^{2}
		\label{eq:gradient-descent-review}
	\end{equation}
	\item $\dots$
\end{itemize}

\section{Chain Rule}\label{sec:chain-rule}
\begin{itemize}
	\item \data{$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}$}
	\item \data{$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y_{1}}\frac{\partial y_{1}}{\partial x} + \frac{\partial z}{\partial y_{2}}\frac{\partial y_{2}}{\partial x}$}
\end{itemize}

\section{Backward Pass}\label{sec:backward-pass}
\begin{itemize}
	\item Function 1 (\emph{error}):
	\begin{equation*}
	\begin{aligned}
		\frac{\partial E_{h}}{\partial o_{h}} &= -(y_{h} - o_{h})\\
		&= o_{h} - y_{h}
	\end{aligned}
	\end{equation*}
	\item Function 2 (\emph{differentiable activation function}):
	\begin{equation*}
	\begin{aligned}
		\frac{\partial o_{h}}{\partial net_{h}} &= \frac{e^{-net_{h}}}{\left( 1 + e^{-net_{h}} \right)^{2}}\\
		&= \frac{1}{1 + e^{-net_{h}}} \times \left( 1 - \frac{1}{1 + e^{-net_{h}}} \right)\\
		&= o_{h}(1 - o_{h})
	\end{aligned}
	\end{equation*}
	\item Function 3 (\emph{linear} gate):
	\begin{equation*}
	\begin{aligned}
		\frac{\partial net_{h}}{\partial w_{jh}} = x_{j}
	\end{aligned}
	\end{equation*}
\end{itemize}

Output neuron backpropagation error:
\begin{equation}
	\begin{aligned}
		\delta_{1}^{y} &= \frac{\partial E_{1}}{\partial o_{1}^{y}}\frac{\partial o_{1}^{y}}{\partial net_{1}}\\
		&= (o_{1}^{y} - y_{1})o_{1}^{y}(1 - o_{1}^{y})
	\end{aligned}
	\label{eq:output-backprop-error}
\end{equation}

Hidden neuron backpropagation error:
\begin{equation}
	\begin{aligned}
		\delta_{1}^{h_{2}} &= \delta_{1}^{h_{2}}(1 - o_{1}^{h_{2}}) \sum_{m=1}^{3} \delta_{m}^{y}w_{h\frac{1}{2}y_{m}}
	\end{aligned}
	\label{eq:hidden-backprop-error}
\end{equation}

Weight updates:
\begin{equation*}
\begin{aligned}
	w_{h\frac{1}{2}y_{1}} &= w_{h\frac{1}{2}y_{1}} - \eta \delta_{1}^{y} o_{1}^{h_{2}}\\
	w_{h\frac{1}{1}h\frac{1}{2}} &= w_{h\frac{1}{1}\frac{1}{2}} - \eta \delta_{1}^{h_{2}} o_{1}^{h_{1}}\\
\end{aligned}
\end{equation*}

\section{Backpropagation using Sigmoid}\label{sec:backpropagation-using-sigmoid}
Repeat for each training example until end of training epoch:
\begin{enumerate}
	\item In the forward pass, compute the \emph{outputs} \data{$o$} of each neuron
	\begin{enumerate}
		\item The outputs for the \emph{input layer neurons} stay unchanged.
		\item The outputs for the \emph{hidden} and \emph{output layer neurons} are computed using the \emph{sigmoid} activation function.
	\end{enumerate}
	\item In the \emph{backward pass}, \emph{propagate} the \emph{errors} \data{$\delta$} from \emph{output layer}
	\begin{enumerate}
		\item For each \emph{output} neuron \data{$k: \delta_{k} = (o_{k} - y_{k})o_{k}(1 - o_{K})$}
		\item For each \emph{hidden} neuron: \data{$h: \delta_{h} = o_{h}(1 - o_{h}) \sum_{m=1}^{|h'|} \delta_{m}w_{hm}$}, where \data{$h'$} is the \emph{subsequent layer}.
	\end{enumerate}
	\item Update every \emph{weights} \data{$w_{ij} \gets w_{ij} - \eta\delta_{j} o_{i}$} where \data{$w_{ij}$} denotes the \emph{weight} between \emph{nodes} \data{$i$} and \data{$j$}.
\end{enumerate}

\section{Backpropagation Example}\label{sec:backpropagation-example}
\begin{itemize}
	\item \data{$x$} are the \emph{inputs}, \data{$h$} are the \emph{neurons} with \emph{sigmoid activation}, \data{$y$} are the \emph{outputs}.
	\item The \emph{initial weights} of all the layers are shown in the figure.
	\item Our objective is to learn $\dots$ \data{$\eta=1$}.
\end{itemize}

\section{Backpropagation with Batch Gradient Descent}\label{sec:backpropagation-with-batch-gradient-descent}
\begin{itemize}
	\item The \emph{backpropagation} algorithm we covered so far, as well as the example we practiced, both \emph{used stochastic gradient descent} for weight updates i.e., the \emph{weights} are \emph{updated} for \emph{every training instances}.
	\item In \emph{backpropagation} using bgd, the \emph{weights} are \emph{updated} $\dots$
\end{itemize}

\section{Training}\label{sec:training}
\emph{Backpropagation} is a gradient estimation method used to train NNs --
\begin{itemize}
	\item \emph{No guarantee} of \emph{convergence} since NNs form \emph{non-convex functions} with \emph{multiple local minima}.
	\item \emph{Many epochs} (tens of thousands) may be needed for adequate training.
	\item \emph{Large data sets} may require many hours of CPU\@.
	\item \emph{Termination criteria}: Number of epochs, threshold on training set error, early stopping, increased error on validation set.
	\item
	\item \emph{Underfitting} --
	\begin{itemize}
		\item Using \emph{too few} hidden neurons in the NN\@.
		\item \emph{Inadequate} or less \emph{data} to train the NN on.
	\end{itemize}
	\item \emph{Overfitting} --
	\begin{itemize}
		\item Training the NN over \emph{too manu epochs}.
		\item Using \emph{too many hidden layers} in the NN\@.
		\item Using \emph{too many neurons} in a hidden layer in the NN\@.
	\end{itemize}
\end{itemize}

\section{Dropout}\label{sec:dropout}
\begin{itemize}
	\item \emph{Large weights} in NN are a sign or a more \emph{complex network} that has overfit the training data.
	\item Probabilistically \emph{dropping out nodes} in the network is a simple and effective regularization method.
\end{itemize}

\section{Exploding Gradient Problem}\label{sec:exploding-gradient-problem}

\section{Dying ReLU Problem}\label{sec:dying-relu-problem}
\begin{itemize}
	\item A \emph{dying RELU} always \emph{outputs} the same value i.e., \data{$0$} on any input value.
	\item This condition is known as the \emph{dead state} of the ReLU \emph{neurons}.
	\item In this state, it is \emph{difficult to recover} because the \emph{gradient} of \data{$0$} is \data{$0$}.
	\item This becomes a problem when most of the \emph{input ranges} are \emph{negative}, or the \emph{derivative} of the \emph{ReLU function} is \data{$0$}.
	\item Can be caused by a \emph{high learning rate} or a \emph{large negative bias.}
	\item Using \emph{Leaky ReLU} instead can resolve the dying ReLU problem.
	\begin{equation}
		\begin{aligned}
			g(x) &= \max\{ 0.01x, x\}\\
			g'(x) &= \left\{ \begin{array}{ll}
				1, & x \geq 0\\
				0.01, & x < 0\\
			\end{array} \right.
		\end{aligned}
		\label{eq:leaky-relu}
	\end{equation}
\end{itemize}

\end{document}